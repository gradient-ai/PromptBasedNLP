{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install openprompt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We simulate a 2-class problem with classes being sports and health. We also define three input examples for which we are interested in getting the classification labels. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "classes = [\n",
    "    \"Sports\",\n",
    "    \"Health\"\n",
    "]\n",
    "dataset = [\n",
    "    InputExample(\n",
    "        guid=0,\n",
    "        text_a=\"Cricket is a really popular sport in India.\",\n",
    "    ),\n",
    "    InputExample(\n",
    "        guid=1,\n",
    "        text_a=\"Coronavirus is an infectious disease.\",\n",
    "    ),\n",
    "    InputExample(\n",
    "        guid=2,\n",
    "        text_a=\"It's common to get hurt while doing stunts.\",\n",
    "    )\n",
    "]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we load our language model and we choose RoBERTa for our purposes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\n",
    "    \"roberta\", \"roberta-base\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define our template that allows us to put in our input example stored in \"text_a\" variable dynamically. The {\"mask\"} token is what the model fills-in. Feel free to check out How to Write a Template? for more detailed steps in designing yours."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "promptTemplate = ManualTemplate(\n",
    "    text='{\"placeholder\":\"text_a\"} It was {\"mask\"}',\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define verbalizer that allows us to project our model's prediction to our pre-defined class labels. Feel free to check out How to Write a Verbalizer? for more detailed steps in designing yours."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words={\n",
    "        \"Health\": [\"Medicine\"],\n",
    "        \"Sports\": [\"Game\", \"Play\"],\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create our prompt model for classification by passing in necessary parameters like templates, language model and verbalizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from openprompt import PromptForClassification\n",
    "promptModel = PromptForClassification(\n",
    "    template=promptTemplate,\n",
    "    plm=plm,\n",
    "    verbalizer=promptVerbalizer,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create our data loader for sampling mini-batches from a dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from openprompt import PromptDataLoader\n",
    "data_loader = PromptDataLoader(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=promptTemplate,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we set our model in evaluation mode and make prediction for each of the input example in a Masked-language model (MLM) fashion. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "promptModel.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        logits = promptModel(batch)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        print(tokenizer.decode(batch['input_ids'][0],\n",
    "                               skip_special_tokens=True), classes[preds])\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}